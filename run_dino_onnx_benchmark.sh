#!/bin/bash

echo "ğŸš€ DINO ONNX BENCHMARK"
echo "ğŸ”„ Convert DINO models to ONNX for better performance"
echo "ğŸ“Š Compare PyTorch vs ONNX vs ONNX Quantized"
echo "================================================================="

echo "ğŸ”§ Installing ONNX dependencies..."
pip install -q onnx onnxruntime

echo ""
echo "ğŸš€ Running DINO ONNX benchmark..."
python dino_onnx_benchmark.py

echo ""
echo "ğŸ“Š Checking results..."
if [ -f "dino_onnx_benchmark_results.json" ]; then
    echo "âœ… Results saved to: dino_onnx_benchmark_results.json"
    echo "ğŸ“„ Results file size: $(du -h dino_onnx_benchmark_results.json | cut -f1)"
else
    echo "âŒ No results file found!"
fi

echo ""
echo "ğŸ DINO ONNX benchmark complete!"
echo "ğŸ”„ ONNX conversion provides better performance and deployment options"
echo "ğŸ“Š Use ONNX quantized models for the best size/performance balance"
echo "ğŸ¯ ONNX Runtime is often 2-3x faster than PyTorch for inference" 