#!/usr/bin/env python3
"""
Comprehensive LSVL Method Comparison
Compares all three approaches:
1. Cosine similarity (2-c)/2
2. Exponential distance exp(-e) 
3. Enhanced SuperPoint integration
"""

import json
import math

def load_all_reports():
    """Load all simulation reports"""
    reports = {}
    
    # Load cosine similarity report
    try:
        with open("lsvl_simulation_report.json", "r") as f:
            reports['cosine'] = json.load(f)
    except FileNotFoundError:
        print("‚ùå Cosine similarity report not found")
        reports['cosine'] = None
    
    # Load exponential distance report
    try:
        with open("lsvl_exp_distance_report.json", "r") as f:
            reports['exponential'] = json.load(f)
    except FileNotFoundError:
        print("‚ùå Exponential distance report not found")
        reports['exponential'] = None
    
    # Load enhanced SuperPoint report
    try:
        with open("lsvl_enhanced_superpoint_report.json", "r") as f:
            reports['enhanced'] = json.load(f)
    except FileNotFoundError:
        print("‚ùå Enhanced SuperPoint report not found")
        reports['enhanced'] = None
    
    return reports

def calculate_improvement(baseline, new_val):
    """Calculate percentage improvement"""
    if baseline == 0:
        return float('inf') if new_val > 0 else 0
    return ((new_val - baseline) / baseline) * 100

def create_comparison_table(reports):
    """Create comprehensive comparison table"""
    print("\nüìä COMPREHENSIVE LSVL METHOD COMPARISON")
    print("=" * 80)
    
    # Extract final metrics
    methods = []
    metrics = []
    
    if reports['cosine']:
        methods.append("Cosine (2-c)/2")
        metrics.append(reports['cosine']['final_metrics'])
    
    if reports['exponential']:
        methods.append("Exponential exp(-e)")
        metrics.append(reports['exponential']['final_metrics'])
    
    if reports['enhanced']:
        methods.append("Enhanced + SuperPoint")
        metrics.append(reports['enhanced']['final_metrics'])
    
    if not methods:
        print("‚ùå No valid reports found for comparison")
        return
    
    # Print comparison table
    print(f"{'Metric':<25} ", end="")
    for method in methods:
        print(f"{method:<20} ", end="")
    print("Best Method")
    print("-" * 80)
    
    # Compare key metrics
    metric_names = [
        ("Recall@1 (100m)", 'avg_recall_1', True),
        ("Recall@5 (200m)", 'avg_recall_5', True),
        ("Position Error (m)", 'avg_position_error', False),
        ("Confidence Score", 'avg_confidence', True)
    ]
    
    best_methods = {}
    
    for metric_name, metric_key, higher_better in metric_names:
        print(f"{metric_name:<25} ", end="")
        
        values = []
        for i, metric_set in enumerate(metrics):
            value = metric_set[metric_key]
            values.append(value)
            print(f"{value:<20.3f} ", end="")
        
        # Find best method
        if higher_better:
            best_idx = values.index(max(values))
        else:
            best_idx = values.index(min(values))
        
        best_method = methods[best_idx]
        best_methods[metric_name] = best_method
        print(f"‚úÖ {best_method}")
    
    return best_methods

def analyze_method_evolution(reports):
    """Analyze the evolution from basic to enhanced methods"""
    print(f"\nüî¨ METHOD EVOLUTION ANALYSIS")
    print("=" * 60)
    
    if not all(reports.values()):
        print("‚ùå Cannot perform evolution analysis - missing reports")
        return
    
    # Extract baseline (cosine) metrics
    baseline = reports['cosine']['final_metrics']
    exponential = reports['exponential']['final_metrics']
    enhanced = reports['enhanced']['final_metrics']
    
    print(f"\nüìà Performance Evolution:")
    print(f"   Step 1: Cosine Similarity (2-c)/2 - Baseline")
    print(f"   Step 2: Exponential Distance exp(-e) - Core Improvement") 
    print(f"   Step 3: Enhanced + SuperPoint - Advanced Integration")
    
    print(f"\nüéØ Recall@1 Evolution:")
    cosine_r1 = baseline['avg_recall_1']
    exp_r1 = exponential['avg_recall_1']
    enh_r1 = enhanced['avg_recall_1']
    
    exp_improvement = calculate_improvement(cosine_r1, exp_r1)
    enh_improvement = calculate_improvement(cosine_r1, enh_r1)
    
    print(f"   ‚Ä¢ Baseline:     {cosine_r1:.1%}")
    print(f"   ‚Ä¢ Exponential:  {exp_r1:.1%} ({exp_improvement:+.0f}%)")
    print(f"   ‚Ä¢ Enhanced:     {enh_r1:.1%} ({enh_improvement:+.0f}%)")
    
    print(f"\nüìè Position Error Evolution:")
    cosine_err = baseline['avg_position_error']
    exp_err = exponential['avg_position_error']
    enh_err = enhanced['avg_position_error']
    
    exp_err_improvement = calculate_improvement(cosine_err, exp_err)
    enh_err_improvement = calculate_improvement(cosine_err, enh_err)
    
    print(f"   ‚Ä¢ Baseline:     {cosine_err:.1f}m")
    print(f"   ‚Ä¢ Exponential:  {exp_err:.1f}m ({-exp_err_improvement:.0f}% reduction)")
    print(f"   ‚Ä¢ Enhanced:     {enh_err:.1f}m ({-enh_err_improvement:.0f}% reduction)")
    
    print(f"\nüéØ Confidence Evolution:")
    cosine_conf = baseline['avg_confidence']
    exp_conf = exponential['avg_confidence']
    enh_conf = enhanced['avg_confidence']
    
    exp_conf_improvement = calculate_improvement(cosine_conf, exp_conf)
    enh_conf_improvement = calculate_improvement(cosine_conf, enh_conf)
    
    print(f"   ‚Ä¢ Baseline:     {cosine_conf:.3f}")
    print(f"   ‚Ä¢ Exponential:  {exp_conf:.3f} ({exp_conf_improvement:+.0f}%)")
    print(f"   ‚Ä¢ Enhanced:     {enh_conf:.3f} ({enh_conf_improvement:+.0f}%)")

def analyze_superpoint_impact(reports):
    """Analyze the specific impact of SuperPoint integration"""
    print(f"\nüîç SUPERPOINT INTEGRATION IMPACT")
    print("=" * 60)
    
    if not reports['exponential'] or not reports['enhanced']:
        print("‚ùå Cannot analyze SuperPoint impact - missing reports")
        return
    
    exp_metrics = reports['exponential']['final_metrics']
    enh_metrics = reports['enhanced']['final_metrics']
    
    # Compare exponential vs enhanced (SuperPoint impact)
    print(f"\nüìä SuperPoint Enhancement Impact:")
    
    metrics_comparison = [
        ("Recall@1", exp_metrics['avg_recall_1'], enh_metrics['avg_recall_1'], True),
        ("Recall@5", exp_metrics['avg_recall_5'], enh_metrics['avg_recall_5'], True),
        ("Position Error", exp_metrics['avg_position_error'], enh_metrics['avg_position_error'], False),
        ("Confidence", exp_metrics['avg_confidence'], enh_metrics['avg_confidence'], True)
    ]
    
    superpoint_improvements = {}
    
    for metric_name, exp_val, enh_val, higher_better in metrics_comparison:
        improvement = calculate_improvement(exp_val, enh_val)
        
        if higher_better:
            direction = "‚ÜóÔ∏è" if enh_val > exp_val else "‚ÜòÔ∏è"
            improvement_str = f"{improvement:+.1f}%"
        else:
            direction = "‚ÜóÔ∏è" if enh_val < exp_val else "‚ÜòÔ∏è"
            improvement_str = f"{-improvement:.1f}% reduction"
        
        superpoint_improvements[metric_name] = improvement
        
        print(f"   ‚Ä¢ {metric_name:<15}: {exp_val:.3f} ‚Üí {enh_val:.3f} {direction} ({improvement_str})")
    
    # SuperPoint integration analysis
    if 'superpoint_integration' in reports['enhanced']:
        sp_info = reports['enhanced']['superpoint_integration']
        print(f"\nüîß SuperPoint Integration Details:")
        print(f"   ‚Ä¢ Availability: {'‚úÖ' if sp_info['available'] else '‚ùå'}")
        print(f"   ‚Ä¢ Weights Loaded: {'‚úÖ' if sp_info['weights_loaded'] else '‚ùå'}")
        print(f"   ‚Ä¢ Active Rate: {sp_info['active_rate']:.1%}")
    
    return superpoint_improvements

def create_method_summary(reports):
    """Create summary of each method's characteristics"""
    print(f"\nüìã METHOD CHARACTERISTICS SUMMARY")
    print("=" * 60)
    
    methods_info = [
        {
            'name': 'Cosine Similarity (2-c)/2',
            'formula': '(2 - cosine_similarity) / 2',
            'range': '[0.5, 1.0]',
            'behavior': 'Linear scaling',
            'strengths': ['Stable', 'Interpretable', 'Simple'],
            'weaknesses': ['Limited dynamic range', 'Moderate discrimination']
        },
        {
            'name': 'Exponential Distance exp(-e)',
            'formula': 'exp(-embedding_distance)',
            'range': '(0, 1]',
            'behavior': 'Exponential decay',
            'strengths': ['High dynamic range', 'Strong discrimination', 'Better convergence'],
            'weaknesses': ['Sensitive to noise', 'Can be unstable']
        },
        {
            'name': 'Enhanced + SuperPoint',
            'formula': 'exp(-e) √ó exp(-desc_dist) with top-5 strategy',
            'range': '(0, 1]',
            'behavior': 'Multi-modal exponential weighting',
            'strengths': ['Descriptor matching', 'Top-5 strategy', 'Penalty system', 'Best performance'],
            'weaknesses': ['Complex implementation', 'Requires trained SuperPoint']
        }
    ]
    
    for i, method in enumerate(methods_info, 1):
        print(f"\n{i}. {method['name']}")
        print(f"   üìê Formula: {method['formula']}")
        print(f"   üìä Range: {method['range']}")
        print(f"   üîÑ Behavior: {method['behavior']}")
        print(f"   ‚úÖ Strengths: {', '.join(method['strengths'])}")
        print(f"   ‚ö†Ô∏è Weaknesses: {', '.join(method['weaknesses'])}")

def generate_recommendations(reports, best_methods):
    """Generate final recommendations"""
    print(f"\nüéØ FINAL RECOMMENDATIONS")
    print("=" * 60)
    
    if not all(reports.values()):
        print("‚ùå Cannot generate recommendations - missing reports")
        return
    
    # Count wins for each method
    method_wins = {}
    for best_method in best_methods.values():
        method_wins[best_method] = method_wins.get(best_method, 0) + 1
    
    # Find overall winner
    overall_winner = max(method_wins.keys(), key=lambda x: method_wins[x])
    
    print(f"\nüèÜ OVERALL WINNER: {overall_winner}")
    print(f"   Wins in {method_wins[overall_winner]}/4 key metrics")
    
    # Specific recommendations
    print(f"\nüí° SPECIFIC RECOMMENDATIONS:")
    
    if "Enhanced + SuperPoint" in method_wins:
        print(f"   ‚úÖ PRODUCTION DEPLOYMENT:")
        print(f"      ‚Ä¢ Use Enhanced + SuperPoint method")
        print(f"      ‚Ä¢ Provides best overall performance")
        print(f"      ‚Ä¢ Integrates multiple modalities effectively")
        print(f"      ‚Ä¢ Requires trained SuperPoint model")
    
    if "Exponential exp(-e)" in method_wins:
        print(f"   ‚ö° PERFORMANCE OPTIMIZATION:")
        print(f"      ‚Ä¢ Use Exponential Distance method as baseline")
        print(f"      ‚Ä¢ 800% improvement over cosine similarity")
        print(f"      ‚Ä¢ Good balance of performance and complexity")
    
    print(f"   üîß IMPLEMENTATION STRATEGY:")
    print(f"      ‚Ä¢ Start with exponential distance method")
    print(f"      ‚Ä¢ Add SuperPoint integration for production")
    print(f"      ‚Ä¢ Use top-5 candidate strategy")
    print(f"      ‚Ä¢ Implement penalty system for non-top candidates")
    
    # Performance summary
    enh_metrics = reports['enhanced']['final_metrics']
    print(f"\nüìà EXPECTED PERFORMANCE (Enhanced Method):")
    print(f"   ‚Ä¢ Recall@1: {enh_metrics['avg_recall_1']:.1%}")
    print(f"   ‚Ä¢ Recall@5: {enh_metrics['avg_recall_5']:.1%}")
    print(f"   ‚Ä¢ Position Error: {enh_metrics['avg_position_error']:.0f}m")
    print(f"   ‚Ä¢ Confidence: {enh_metrics['avg_confidence']:.1%}")

def main():
    """Main comparison analysis"""
    print("üîç Comprehensive LSVL Method Comparison")
    print("üéØ Analyzing Cosine Similarity vs Exponential Distance vs Enhanced SuperPoint")
    print("=" * 80)
    
    # Load all reports
    reports = load_all_reports()
    
    # Create comparison table
    best_methods = create_comparison_table(reports)
    
    # Analyze method evolution
    analyze_method_evolution(reports)
    
    # Analyze SuperPoint impact
    superpoint_improvements = analyze_superpoint_impact(reports)
    
    # Create method summary
    create_method_summary(reports)
    
    # Generate recommendations
    generate_recommendations(reports, best_methods)
    
    # Save comprehensive comparison
    comparison_data = {
        'comparison_timestamp': reports['enhanced']['simulation_timestamp'] if reports['enhanced'] else "unknown",
        'methods_analyzed': [
            'Cosine Similarity (2-c)/2',
            'Exponential Distance exp(-e)',
            'Enhanced + SuperPoint'
        ],
        'best_methods': best_methods,
        'superpoint_improvements': superpoint_improvements if 'superpoint_improvements' in locals() else {},
        'final_recommendation': 'Enhanced + SuperPoint' if reports['enhanced'] else 'Exponential Distance',
        'summary': {
            'cosine_metrics': reports['cosine']['final_metrics'] if reports['cosine'] else None,
            'exponential_metrics': reports['exponential']['final_metrics'] if reports['exponential'] else None,
            'enhanced_metrics': reports['enhanced']['final_metrics'] if reports['enhanced'] else None
        }
    }
    
    with open("comprehensive_lsvl_comparison.json", "w") as f:
        json.dump(comparison_data, f, indent=2)
    
    print(f"\nüíæ Comprehensive comparison saved: comprehensive_lsvl_comparison.json")
    print(f"\n‚úÖ Analysis Complete!")

if __name__ == "__main__":
    main()