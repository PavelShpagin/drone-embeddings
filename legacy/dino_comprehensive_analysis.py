#!/usr/bin/env python3

import torch
import timm
import time
import numpy as np
import gc
import psutil

def get_model_size_mb(model):
    """Calculate model size in MB based on actual parameters."""
    param_size = 0
    for param in model.parameters():
        param_size += param.nelement() * param.element_size()
    buffer_size = 0
    for buffer in model.buffers():
        buffer_size += buffer.nelement() * buffer.element_size()
    return (param_size + buffer_size) / 1024**2

def benchmark_model_comprehensive(model_name, test_quantization=True):
    """Comprehensive benchmark of a single model with all configurations."""
    print(f"\n{'='*80}")
    print(f"üîç COMPREHENSIVE ANALYSIS: {model_name}")
    print(f"{'='*80}")
    
    results = []
    
    try:
        # Load original model
        print("Loading original model...")
        model = timm.create_model(model_name, pretrained=True)
        model.eval()
        
        # Get base model info
        original_params = sum(p.numel() for p in model.parameters())
        original_size = get_model_size_mb(model)
        
        print(f"üìä Base Model Info:")
        print(f"  Parameters: {original_params:,}")
        print(f"  Actual Size: {original_size:.2f} MB")
        
        # Calculate theoretical sizes
        theoretical_fp32 = original_params * 4 / 1024**2
        theoretical_int8 = original_params * 1 / 1024**2
        theoretical_int4 = original_params * 0.5 / 1024**2
        
        print(f"  Theoretical FP32: {theoretical_fp32:.2f} MB")
        print(f"  Theoretical INT8: {theoretical_int8:.2f} MB")
        print(f"  Theoretical INT4: {theoretical_int4:.2f} MB")
        
        # Performance test
        dummy_input = torch.randn(1, 3, 224, 224)
        
        # Test original model
        print("\nüöÄ Testing Original Model (FP32):")
        fps_original = benchmark_single_config(model, dummy_input, "FP32")
        
        result_original = {
            'model': model_name,
            'config': 'FP32',
            'actual_size': original_size,
            'theoretical_size': theoretical_fp32,
            'fps': fps_original,
            'params': original_params,
            'compression_ratio': 1.0,
            'feasible': fps_original >= 10.0 and original_size <= 100.0
        }
        results.append(result_original)
        
        if test_quantization:
            # Test INT8 quantization
            print("\nüöÄ Testing INT8 Quantization:")
            try:
                model_int8 = torch.quantization.quantize_dynamic(
                    model, {torch.nn.Linear}, dtype=torch.qint8
                )
                int8_size = get_model_size_mb(model_int8)
                fps_int8 = benchmark_single_config(model_int8, dummy_input, "INT8")
                
                result_int8 = {
                    'model': model_name,
                    'config': 'INT8',
                    'actual_size': int8_size,
                    'theoretical_size': theoretical_int8,
                    'fps': fps_int8,
                    'params': original_params,
                    'compression_ratio': original_size / int8_size,
                    'feasible': fps_int8 >= 10.0 and int8_size <= 100.0
                }
                results.append(result_int8)
                
            except Exception as e:
                print(f"‚ùå INT8 quantization failed: {e}")
            
            # Test INT4 (simulated via INT8)
            print("\nüöÄ Testing INT4 Quantization (simulated):")
            try:
                model_int4 = torch.quantization.quantize_dynamic(
                    model, {torch.nn.Linear}, dtype=torch.qint8
                )
                int4_size = get_model_size_mb(model_int4)
                fps_int4 = benchmark_single_config(model_int4, dummy_input, "INT4")
                
                # Simulate INT4 size (approximately half of INT8)
                simulated_int4_size = int4_size * 0.5
                
                result_int4 = {
                    'model': model_name,
                    'config': 'INT4 (simulated)',
                    'actual_size': simulated_int4_size,
                    'theoretical_size': theoretical_int4,
                    'fps': fps_int4,
                    'params': original_params,
                    'compression_ratio': original_size / simulated_int4_size,
                    'feasible': fps_int4 >= 10.0 and simulated_int4_size <= 100.0
                }
                results.append(result_int4)
                
            except Exception as e:
                print(f"‚ùå INT4 quantization failed: {e}")
        
        return results
        
    except Exception as e:
        print(f"‚ùå Error testing {model_name}: {e}")
        return []

def benchmark_single_config(model, input_tensor, config_name):
    """Benchmark a single model configuration."""
    print(f"   Testing {config_name}...")
    
    model.eval()
    
    # Warmup
    for _ in range(5):
        try:
            with torch.no_grad():
                _ = model(input_tensor)
        except:
            pass
    
    # Measure
    times = []
    for _ in range(20):
        start_time = time.time()
        try:
            with torch.no_grad():
                _ = model(input_tensor)
            end_time = time.time()
            times.append(end_time - start_time)
        except:
            times.append(1.0)  # Fallback for failed inference
    
    avg_time = np.mean(times)
    fps = 1.0 / avg_time if avg_time > 0 else 0
    
    print(f"   {config_name}: {fps:.2f} FPS ({avg_time*1000:.2f} ms)")
    
    return fps

def main():
    print("üéØ COMPREHENSIVE DINO/DINOv2 BENCHMARK")
    print("üìä Testing all promising configurations for Raspberry Pi Zero")
    print("üîç Proper size calculations based on ACTUAL models")
    print("=" * 80)
    
    # Define all promising DINO configurations
    dino_models = [
        # Original DINO variants
        'dino_vits16',      # DINO Small, patch 16 (22M params)
        'dino_vits8',       # DINO Small, patch 8 (22M params)
        'dino_vitb16',      # DINO Base, patch 16 (86M params)
        'dino_vitb8',       # DINO Base, patch 8 (86M params)
        
        # DINOv2 variants (Meta's improved version)
        'dinov2_vits14',    # DINOv2 Small, patch 14 (22M params)
        'dinov2_vitb14',    # DINOv2 Base, patch 14 (86M params)
        'dinov2_vitl14',    # DINOv2 Large, patch 14 (300M params)
        'dinov2_vitg14',    # DINOv2 Giant, patch 14 (1.1B params)
    ]
    
    all_results = []
    
    for model_name in dino_models:
        model_results = benchmark_model_comprehensive(model_name)
        all_results.extend(model_results)
    
    # Sort by FPS (descending)
    all_results.sort(key=lambda x: x['fps'], reverse=True)
    
    # Filter feasible models
    feasible_models = [r for r in all_results if r['feasible']]
    
    print(f"\n{'='*100}")
    print("üìà COMPREHENSIVE RESULTS SUMMARY")
    print(f"{'='*100}")
    
    print(f"\nAll Configurations Tested ({len(all_results)}):")
    print(f"{'Model':<20} {'Config':<15} {'Actual(MB)':<12} {'Theoretical(MB)':<15} {'FPS':<8} {'Feasible':<10}")
    print("-" * 100)
    
    for result in all_results:
        feasible_str = "‚úÖ" if result['feasible'] else "‚ùå"
        print(f"{result['model']:<20} {result['config']:<15} {result['actual_size']:<12.2f} "
              f"{result['theoretical_size']:<15.2f} {result['fps']:<8.2f} {feasible_str:<10}")
    
    print(f"\n{'='*100}")
    print(f"üèÜ FEASIBLE MODELS FOR RASPBERRY PI ZERO ({len(feasible_models)})")
    print(f"{'='*100}")
    
    if feasible_models:
        print(f"{'Rank':<5} {'Model':<20} {'Config':<15} {'Size(MB)':<10} {'FPS':<8} {'Compression':<12}")
        print("-" * 85)
        
        for i, result in enumerate(feasible_models[:10], 1):
            print(f"{i:<5} {result['model']:<20} {result['config']:<15} "
                  f"{result['actual_size']:<10.2f} {result['fps']:<8.2f} {result['compression_ratio']:<12.2f}x")
    else:
        print("‚ùå No models meet Pi Zero feasibility criteria!")
    
    print(f"\n{'='*100}")
    print("üéØ ANALYSIS & RECOMMENDATIONS")
    print(f"{'='*100}")
    
    if feasible_models:
        # Best overall
        best = feasible_models[0]
        print(f"ü•á BEST OVERALL: {best['model']} ({best['config']})")
        print(f"   Size: {best['actual_size']:.2f} MB")
        print(f"   FPS: {best['fps']:.2f}")
        print(f"   Compression: {best['compression_ratio']:.2f}x")
        
        # Smallest
        smallest = min(feasible_models, key=lambda x: x['actual_size'])
        print(f"\nüíæ SMALLEST: {smallest['model']} ({smallest['config']})")
        print(f"   Size: {smallest['actual_size']:.2f} MB")
        print(f"   FPS: {smallest['fps']:.2f}")
        
        # Largest feasible
        largest = max(feasible_models, key=lambda x: x['actual_size'])
        print(f"\nüî• LARGEST FEASIBLE: {largest['model']} ({largest['config']})")
        print(f"   Size: {largest['actual_size']:.2f} MB")
        print(f"   FPS: {largest['fps']:.2f}")
        print(f"   Parameters: {largest['params']:,}")
    
    # Model family analysis
    print(f"\nüìä MODEL FAMILY ANALYSIS:")
    dino_results = [r for r in all_results if r['model'].startswith('dino_')]
    dinov2_results = [r for r in all_results if r['model'].startswith('dinov2_')]
    
    print(f"DINO models: {len(dino_results)} configurations")
    print(f"DINOv2 models: {len(dinov2_results)} configurations")
    
    dino_feasible = [r for r in dino_results if r['feasible']]
    dinov2_feasible = [r for r in dinov2_results if r['feasible']]
    
    print(f"DINO feasible: {len(dino_feasible)}")
    print(f"DINOv2 feasible: {len(dinov2_feasible)}")
    
    # Expected theoretical sizes
    print(f"\nüìê THEORETICAL SIZE BREAKDOWN:")
    print("Model sizes based on parameter counts:")
    print("- DINO-S/DINOv2-S (22M params): FP32=88MB, INT8=22MB, INT4=11MB")
    print("- DINO-B/DINOv2-B (86M params): FP32=344MB, INT8=86MB, INT4=43MB")
    print("- DINOv2-L (300M params): FP32=1200MB, INT8=300MB, INT4=150MB")
    print("- DINOv2-G (1.1B params): FP32=4400MB, INT8=1100MB, INT4=550MB")
    
    print(f"\n‚úÖ COMPREHENSIVE ANALYSIS COMPLETE")
    print(f"üéØ Focus on DINO-S and DINOv2-S with INT8/INT4 quantization")
    print(f"üí° Largest feasible: DINOv2-B with INT4 quantization (‚âà43MB)")

if __name__ == "__main__":
    main() 