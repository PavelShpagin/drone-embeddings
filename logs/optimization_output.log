Using cache found in /home/pavel/.cache/torch/hub/facebookresearch_dinov2_main
PyTorch Model Optimizer for Raspberry Pi Zero
==================================================
Loading models...
✓ SuperPoint loaded
✓ MobileNetV2 loaded
✓ MobileNetV3 loaded
✓ EfficientNet-B0 loaded
✓ ResNet50 loaded
✓ DINOv2 loaded
Successfully loaded 6 models

Starting model optimization...

[1/6] Processing superpoint...
  Original model:
    Benchmarking superpoint_original...
  INT8 quantization:
  Quantizing to INT8...
    Benchmarking superpoint_int8...
    Warmup failed for superpoint_int8: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1972 [kernel]
QuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:391 [kernel]
BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]
Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]
AutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]
AutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]
AutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]
AutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]
AutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]
AutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]
AutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]
AutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]
AutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]
Tracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]
AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]
AutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]
AutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]
PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]
PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]

  Mobile optimization:
  Optimizing for mobile...
    Benchmarking superpoint_mobile...

[2/6] Processing mobilenetv2...
  Original model:
    Benchmarking mobilenetv2_original...
  INT8 quantization:
  Quantizing to INT8...
    Benchmarking mobilenetv2_int8...
    Warmup failed for mobilenetv2_int8: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].

Meta: registered at ../aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]
QuantizedCPU: registered at ../aten/src/ATen/native/quantized/cpu/qconv.cpp:1972 [kernel]
QuantizedCUDA: registered at ../aten/src/ATen/native/quantized/cudnn/Conv.cpp:391 [kernel]
BackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]
Python: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]
FuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]
Functionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]
Named: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]
Conjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]
Negative: registered at ../aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]
ZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]
ADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:96 [backend fallback]
AutogradOther: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]
AutogradCPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]
AutogradCUDA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]
AutogradXLA: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]
AutogradMPS: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]
AutogradXPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]
AutogradHPU: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]
AutogradLazy: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]
AutogradMeta: registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]
Tracer: registered at ../torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]
AutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:321 [backend fallback]
AutocastXPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:463 [backend fallback]
AutocastMPS: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:209 [backend fallback]
AutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:165 [backend fallback]
FuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]
BatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]
FuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]
Batched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]
VmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]
FuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]
PythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]
FuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]
PreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]
PythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]

  Mobile optimization:
  Optimizing for mobile...
    Benchmarking mobilenetv2_mobile...

[3/6] Processing mobilenetv3...
  Original model:
    Benchmarking mobilenetv3_original...
  INT8 quantization:
  Quantizing to INT8...
Error preparing mobilenetv3 for quantization: MobileNetV3.__init__() missing 2 required positional arguments: 'inverted_residual_setting' and 'last_channel'
  Mobile optimization:
  Optimizing for mobile...
    Benchmarking mobilenetv3_mobile...

[4/6] Processing efficientnet...
  Original model:
    Benchmarking efficientnet_original...
  INT8 quantization:
  Quantizing to INT8...
Error preparing efficientnet for quantization: EfficientNet.__init__() missing 1 required positional argument: 'block_args'
  Mobile optimization:
  Optimizing for mobile...
    Benchmarking efficientnet_mobile...

[5/6] Processing resnet50...
  Original model:
    Benchmarking resnet50_original...
  INT8 quantization:
  Quantizing to INT8...
Error preparing resnet50 for quantization: ResNet.__init__() missing 2 required positional arguments: 'block' and 'layers'
  Mobile optimization:
  Optimizing for mobile...
    Benchmarking resnet50_mobile...

[6/6] Processing dinov2...
  Original model:
    Benchmarking dinov2_original...
    Warmup failed for dinov2_original: No operator found for `memory_efficient_attention_forward` with inputs:
     query       : shape=(1, 1370, 6, 64) (torch.float32)
     key         : shape=(1, 1370, 6, 64) (torch.float32)
     value       : shape=(1, 1370, 6, 64) (torch.float32)
     attn_bias   : <class 'NoneType'>
     p           : 0.0
`fa2F@v2.5.7-pt` is not supported because:
    device=cpu (supported: {'cuda'})
    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})
`cutlassF-pt` is not supported because:
    device=cpu (supported: {'cuda'})
  INT8 quantization:
  Quantizing to INT8...
Error preparing dinov2 for quantization: Error(s) in loading state_dict for DinoVisionTransformer:
	Missing key(s) in state_dict: "blocks.0.0.norm1.weight", "blocks.0.0.norm1.bias", "blocks.0.0.attn.qkv.weight", "blocks.0.0.attn.qkv.bias", "blocks.0.0.attn.proj.weight", "blocks.0.0.attn.proj.bias", "blocks.0.0.norm2.weight", "blocks.0.0.norm2.bias", "blocks.0.0.mlp.fc1.weight", "blocks.0.0.mlp.fc1.bias", "blocks.0.0.mlp.fc2.weight", "blocks.0.0.mlp.fc2.bias", "blocks.0.1.norm1.weight", "blocks.0.1.norm1.bias", "blocks.0.1.attn.qkv.weight", "blocks.0.1.attn.qkv.bias", "blocks.0.1.attn.proj.weight", "blocks.0.1.attn.proj.bias", "blocks.0.1.norm2.weight", "blocks.0.1.norm2.bias", "blocks.0.1.mlp.fc1.weight", "blocks.0.1.mlp.fc1.bias", "blocks.0.1.mlp.fc2.weight", "blocks.0.1.mlp.fc2.bias", "blocks.0.2.norm1.weight", "blocks.0.2.norm1.bias", "blocks.0.2.attn.qkv.weight", "blocks.0.2.attn.qkv.bias", "blocks.0.2.attn.proj.weight", "blocks.0.2.attn.proj.bias", "blocks.0.2.norm2.weight", "blocks.0.2.norm2.bias", "blocks.0.2.mlp.fc1.weight", "blocks.0.2.mlp.fc1.bias", "blocks.0.2.mlp.fc2.weight", "blocks.0.2.mlp.fc2.bias", "blocks.0.3.norm1.weight", "blocks.0.3.norm1.bias", "blocks.0.3.attn.qkv.weight", "blocks.0.3.attn.qkv.bias", "blocks.0.3.attn.proj.weight", "blocks.0.3.attn.proj.bias", "blocks.0.3.norm2.weight", "blocks.0.3.norm2.bias", "blocks.0.3.mlp.fc1.weight", "blocks.0.3.mlp.fc1.bias", "blocks.0.3.mlp.fc2.weight", "blocks.0.3.mlp.fc2.bias", "blocks.0.4.norm1.weight", "blocks.0.4.norm1.bias", "blocks.0.4.attn.qkv.weight", "blocks.0.4.attn.qkv.bias", "blocks.0.4.attn.proj.weight", "blocks.0.4.attn.proj.bias", "blocks.0.4.norm2.weight", "blocks.0.4.norm2.bias", "blocks.0.4.mlp.fc1.weight", "blocks.0.4.mlp.fc1.bias", "blocks.0.4.mlp.fc2.weight", "blocks.0.4.mlp.fc2.bias", "blocks.0.5.norm1.weight", "blocks.0.5.norm1.bias", "blocks.0.5.attn.qkv.weight", "blocks.0.5.attn.qkv.bias", "blocks.0.5.attn.proj.weight", "blocks.0.5.attn.proj.bias", "blocks.0.5.norm2.weight", "blocks.0.5.norm2.bias", "blocks.0.5.mlp.fc1.weight", "blocks.0.5.mlp.fc1.bias", "blocks.0.5.mlp.fc2.weight", "blocks.0.5.mlp.fc2.bias", "blocks.0.6.norm1.weight", "blocks.0.6.norm1.bias", "blocks.0.6.attn.qkv.weight", "blocks.0.6.attn.qkv.bias", "blocks.0.6.attn.proj.weight", "blocks.0.6.attn.proj.bias", "blocks.0.6.norm2.weight", "blocks.0.6.norm2.bias", "blocks.0.6.mlp.fc1.weight", "blocks.0.6.mlp.fc1.bias", "blocks.0.6.mlp.fc2.weight", "blocks.0.6.mlp.fc2.bias", "blocks.0.7.norm1.weight", "blocks.0.7.norm1.bias", "blocks.0.7.attn.qkv.weight", "blocks.0.7.attn.qkv.bias", "blocks.0.7.attn.proj.weight", "blocks.0.7.attn.proj.bias", "blocks.0.7.norm2.weight", "blocks.0.7.norm2.bias", "blocks.0.7.mlp.fc1.weight", "blocks.0.7.mlp.fc1.bias", "blocks.0.7.mlp.fc2.weight", "blocks.0.7.mlp.fc2.bias", "blocks.0.8.norm1.weight", "blocks.0.8.norm1.bias", "blocks.0.8.attn.qkv.weight", "blocks.0.8.attn.qkv.bias", "blocks.0.8.attn.proj.weight", "blocks.0.8.attn.proj.bias", "blocks.0.8.norm2.weight", "blocks.0.8.norm2.bias", "blocks.0.8.mlp.fc1.weight", "blocks.0.8.mlp.fc1.bias", "blocks.0.8.mlp.fc2.weight", "blocks.0.8.mlp.fc2.bias", "blocks.0.9.norm1.weight", "blocks.0.9.norm1.bias", "blocks.0.9.attn.qkv.weight", "blocks.0.9.attn.qkv.bias", "blocks.0.9.attn.proj.weight", "blocks.0.9.attn.proj.bias", "blocks.0.9.norm2.weight", "blocks.0.9.norm2.bias", "blocks.0.9.mlp.fc1.weight", "blocks.0.9.mlp.fc1.bias", "blocks.0.9.mlp.fc2.weight", "blocks.0.9.mlp.fc2.bias", "blocks.0.10.norm1.weight", "blocks.0.10.norm1.bias", "blocks.0.10.attn.qkv.weight", "blocks.0.10.attn.qkv.bias", "blocks.0.10.attn.proj.weight", "blocks.0.10.attn.proj.bias", "blocks.0.10.norm2.weight", "blocks.0.10.norm2.bias", "blocks.0.10.mlp.fc1.weight", "blocks.0.10.mlp.fc1.bias", "blocks.0.10.mlp.fc2.weight", "blocks.0.10.mlp.fc2.bias", "blocks.0.11.norm1.weight", "blocks.0.11.norm1.bias", "blocks.0.11.attn.qkv.weight", "blocks.0.11.attn.qkv.bias", "blocks.0.11.attn.proj.weight", "blocks.0.11.attn.proj.bias", "blocks.0.11.norm2.weight", "blocks.0.11.norm2.bias", "blocks.0.11.mlp.fc1.weight", "blocks.0.11.mlp.fc1.bias", "blocks.0.11.mlp.fc2.weight", "blocks.0.11.mlp.fc2.bias". 
	Unexpected key(s) in state_dict: "blocks.1.norm1.weight", "blocks.1.norm1.bias", "blocks.1.attn.qkv.weight", "blocks.1.attn.qkv.bias", "blocks.1.attn.proj.weight", "blocks.1.attn.proj.bias", "blocks.1.ls1.gamma", "blocks.1.norm2.weight", "blocks.1.norm2.bias", "blocks.1.mlp.fc1.weight", "blocks.1.mlp.fc1.bias", "blocks.1.mlp.fc2.weight", "blocks.1.mlp.fc2.bias", "blocks.1.ls2.gamma", "blocks.2.norm1.weight", "blocks.2.norm1.bias", "blocks.2.attn.qkv.weight", "blocks.2.attn.qkv.bias", "blocks.2.attn.proj.weight", "blocks.2.attn.proj.bias", "blocks.2.ls1.gamma", "blocks.2.norm2.weight", "blocks.2.norm2.bias", "blocks.2.mlp.fc1.weight", "blocks.2.mlp.fc1.bias", "blocks.2.mlp.fc2.weight", "blocks.2.mlp.fc2.bias", "blocks.2.ls2.gamma", "blocks.3.norm1.weight", "blocks.3.norm1.bias", "blocks.3.attn.qkv.weight", "blocks.3.attn.qkv.bias", "blocks.3.attn.proj.weight", "blocks.3.attn.proj.bias", "blocks.3.ls1.gamma", "blocks.3.norm2.weight", "blocks.3.norm2.bias", "blocks.3.mlp.fc1.weight", "blocks.3.mlp.fc1.bias", "blocks.3.mlp.fc2.weight", "blocks.3.mlp.fc2.bias", "blocks.3.ls2.gamma", "blocks.4.norm1.weight", "blocks.4.norm1.bias", "blocks.4.attn.qkv.weight", "blocks.4.attn.qkv.bias", "blocks.4.attn.proj.weight", "blocks.4.attn.proj.bias", "blocks.4.ls1.gamma", "blocks.4.norm2.weight", "blocks.4.norm2.bias", "blocks.4.mlp.fc1.weight", "blocks.4.mlp.fc1.bias", "blocks.4.mlp.fc2.weight", "blocks.4.mlp.fc2.bias", "blocks.4.ls2.gamma", "blocks.5.norm1.weight", "blocks.5.norm1.bias", "blocks.5.attn.qkv.weight", "blocks.5.attn.qkv.bias", "blocks.5.attn.proj.weight", "blocks.5.attn.proj.bias", "blocks.5.ls1.gamma", "blocks.5.norm2.weight", "blocks.5.norm2.bias", "blocks.5.mlp.fc1.weight", "blocks.5.mlp.fc1.bias", "blocks.5.mlp.fc2.weight", "blocks.5.mlp.fc2.bias", "blocks.5.ls2.gamma", "blocks.6.norm1.weight", "blocks.6.norm1.bias", "blocks.6.attn.qkv.weight", "blocks.6.attn.qkv.bias", "blocks.6.attn.proj.weight", "blocks.6.attn.proj.bias", "blocks.6.ls1.gamma", "blocks.6.norm2.weight", "blocks.6.norm2.bias", "blocks.6.mlp.fc1.weight", "blocks.6.mlp.fc1.bias", "blocks.6.mlp.fc2.weight", "blocks.6.mlp.fc2.bias", "blocks.6.ls2.gamma", "blocks.7.norm1.weight", "blocks.7.norm1.bias", "blocks.7.attn.qkv.weight", "blocks.7.attn.qkv.bias", "blocks.7.attn.proj.weight", "blocks.7.attn.proj.bias", "blocks.7.ls1.gamma", "blocks.7.norm2.weight", "blocks.7.norm2.bias", "blocks.7.mlp.fc1.weight", "blocks.7.mlp.fc1.bias", "blocks.7.mlp.fc2.weight", "blocks.7.mlp.fc2.bias", "blocks.7.ls2.gamma", "blocks.8.norm1.weight", "blocks.8.norm1.bias", "blocks.8.attn.qkv.weight", "blocks.8.attn.qkv.bias", "blocks.8.attn.proj.weight", "blocks.8.attn.proj.bias", "blocks.8.ls1.gamma", "blocks.8.norm2.weight", "blocks.8.norm2.bias", "blocks.8.mlp.fc1.weight", "blocks.8.mlp.fc1.bias", "blocks.8.mlp.fc2.weight", "blocks.8.mlp.fc2.bias", "blocks.8.ls2.gamma", "blocks.9.norm1.weight", "blocks.9.norm1.bias", "blocks.9.attn.qkv.weight", "blocks.9.attn.qkv.bias", "blocks.9.attn.proj.weight", "blocks.9.attn.proj.bias", "blocks.9.ls1.gamma", "blocks.9.norm2.weight", "blocks.9.norm2.bias", "blocks.9.mlp.fc1.weight", "blocks.9.mlp.fc1.bias", "blocks.9.mlp.fc2.weight", "blocks.9.mlp.fc2.bias", "blocks.9.ls2.gamma", "blocks.10.norm1.weight", "blocks.10.norm1.bias", "blocks.10.attn.qkv.weight", "blocks.10.attn.qkv.bias", "blocks.10.attn.proj.weight", "blocks.10.attn.proj.bias", "blocks.10.ls1.gamma", "blocks.10.norm2.weight", "blocks.10.norm2.bias", "blocks.10.mlp.fc1.weight", "blocks.10.mlp.fc1.bias", "blocks.10.mlp.fc2.weight", "blocks.10.mlp.fc2.bias", "blocks.10.ls2.gamma", "blocks.11.norm1.weight", "blocks.11.norm1.bias", "blocks.11.attn.qkv.weight", "blocks.11.attn.qkv.bias", "blocks.11.attn.proj.weight", "blocks.11.attn.proj.bias", "blocks.11.ls1.gamma", "blocks.11.norm2.weight", "blocks.11.norm2.bias", "blocks.11.mlp.fc1.weight", "blocks.11.mlp.fc1.bias", "blocks.11.mlp.fc2.weight", "blocks.11.mlp.fc2.bias", "blocks.11.ls2.gamma", "blocks.0.norm1.weight", "blocks.0.norm1.bias", "blocks.0.attn.qkv.weight", "blocks.0.attn.qkv.bias", "blocks.0.attn.proj.weight", "blocks.0.attn.proj.bias", "blocks.0.ls1.gamma", "blocks.0.norm2.weight", "blocks.0.norm2.bias", "blocks.0.mlp.fc1.weight", "blocks.0.mlp.fc1.bias", "blocks.0.mlp.fc2.weight", "blocks.0.mlp.fc2.bias", "blocks.0.ls2.gamma". 
	size mismatch for cls_token: copying a param with shape torch.Size([1, 1, 384]) from checkpoint, the shape in current model is torch.Size([1, 1, 768]).
	size mismatch for pos_embed: copying a param with shape torch.Size([1, 1370, 384]) from checkpoint, the shape in current model is torch.Size([1, 197, 768]).
	size mismatch for mask_token: copying a param with shape torch.Size([1, 384]) from checkpoint, the shape in current model is torch.Size([1, 768]).
	size mismatch for patch_embed.proj.weight: copying a param with shape torch.Size([384, 3, 14, 14]) from checkpoint, the shape in current model is torch.Size([768, 3, 16, 16]).
	size mismatch for patch_embed.proj.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for norm.weight: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for norm.bias: copying a param with shape torch.Size([384]) from checkpoint, the shape in current model is torch.Size([768]).
  Mobile optimization:
  Optimizing for mobile...
    Error optimizing dinov2 for mobile: No operator found for `memory_efficient_attention_forward` with inputs:
     query       : shape=(tensor(1), tensor(1370), tensor(6), tensor(64)) (torch.float32)
     key         : shape=(tensor(1), tensor(1370), tensor(6), tensor(64)) (torch.float32)
     value       : shape=(tensor(1), tensor(1370), tensor(6), tensor(64)) (torch.float32)
     attn_bias   : <class 'NoneType'>
     p           : 0.0
`fa2F@v2.5.7-pt` is not supported because:
    device=cpu (supported: {'cuda'})
    dtype=torch.float32 (supported: {torch.float16, torch.bfloat16})
`cutlassF-pt` is not supported because:
    device=cpu (supported: {'cuda'})

Optimization complete! Processed 10 model variants.

Results saved to: optimization_results/optimization_results_20250705_073502.json

================================================================================
OPTIMIZATION SUMMARY
================================================================================

superpoint_original:
  Size: 4.96 MB
  Avg Inference: 36.63 ms
  Throughput: 27.30 FPS
  Input Size: (1, 224, 224)

superpoint_mobile:
  Size: 0.00 MB
  Avg Inference: 75.90 ms
  Throughput: 13.18 FPS
  Input Size: (1, 224, 224)

mobilenetv2_original:
  Size: 13.50 MB
  Avg Inference: 17.39 ms
  Throughput: 57.49 FPS
  Input Size: (3, 224, 224)

mobilenetv2_mobile:
  Size: 0.00 MB
  Avg Inference: 329.28 ms
  Throughput: 3.04 FPS
  Input Size: (3, 224, 224)

mobilenetv3_original:
  Size: 21.01 MB
  Avg Inference: 16.78 ms
  Throughput: 59.60 FPS
  Input Size: (3, 224, 224)

mobilenetv3_mobile:
  Size: 0.00 MB
  Avg Inference: 281.53 ms
  Throughput: 3.55 FPS
  Input Size: (3, 224, 224)

efficientnet_original:
  Size: 20.33 MB
  Avg Inference: 22.74 ms
  Throughput: 43.98 FPS
  Input Size: (3, 224, 224)

efficientnet_mobile:
  Size: 0.00 MB
  Avg Inference: 416.17 ms
  Throughput: 2.40 FPS
  Input Size: (3, 224, 224)

resnet50_original:
  Size: 97.70 MB
  Avg Inference: 53.88 ms
  Throughput: 18.56 FPS
  Input Size: (3, 224, 224)

resnet50_mobile:
  Size: 0.00 MB
  Avg Inference: 389.16 ms
  Throughput: 2.57 FPS
  Input Size: (3, 224, 224)

================================================================================
OPTIMIZATION COMPLETE!
================================================================================

Check the optimization_results/ directory for detailed results.
